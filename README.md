# Machine Learning From Scratch ğŸ§ 

A comprehensive collection of machine learning algorithms implemented from scratch using Python. This repository contains implementations of fundamental ML algorithms along with practical examples, datasets, and detailed explanations.

## ğŸŒŸ Overview

This project demonstrates the implementation of core machine learning algorithms from the ground up, providing educational insights into how these algorithms work internally. Each algorithm is accompanied by practical examples, real-world datasets, and comprehensive documentation.

## ğŸ“š Algorithms Covered

### 1. **Linear Regression** ğŸ“ˆ
- Implementation of simple and multiple linear regression
- Data preprocessing techniques
- Real-world datasets: Tesla stock data, fish measurements, insurance data
- Tutorial materials and assignments

### 2. **Logistic Regression** ğŸ”„
- Binary classification implementation
- Sigmoid function and cost optimization
- Practical examples and case studies

### 3. **K-Nearest Neighbors (KNN)** ğŸ¯
- Distance-based classification algorithm
- Implementation with different distance metrics
- Parameter tuning and optimization

### 4. **Perceptron Learning Algorithm (PLA)** ğŸ§®
- Linear classifier implementation
- Convergence analysis
- Practical applications and experiments

### 5. **Support Vector Machine (SVM)** âš”ï¸
- **Hard Margin SVM**: Linear separable classification
- **Soft Margin SVM**: Handling non-separable data
- **Kernel SVM**: Non-linear classification using kernels
- **Multi-class SVM**: Extension to multiple classes
- Comprehensive formula documentation

### 6. **Softmax Regression** ğŸ²
- Multi-class classification implementation
- Probability distribution modeling
- TensorFlow integration examples

### 7. **Bias-Variance Tradeoff** âš–ï¸
- Understanding model complexity
- Overfitting and underfitting analysis
- Model selection strategies

## ğŸ—ï¸ Project Structure

```
Machine-Learning-From-Scratch/
â”œâ”€â”€ LinearRegression/
â”‚   â”œâ”€â”€ 22520834_LinearRegression_assignment.ipynb
â”‚   â”œâ”€â”€ Data_Preprocessing_tutorial.pdf
â”‚   â”œâ”€â”€ Linear_Regression_tutorial.pdf
â”‚   â”œâ”€â”€ fish.csv
â”‚   â”œâ”€â”€ insurance.csv
â”‚   â””â”€â”€ tesla_stock.csv
â”œâ”€â”€ Logistic Regression/
â”‚   â””â”€â”€ LogisticRegression.ipynb
â”œâ”€â”€ KNN/
â”‚   â””â”€â”€ KNN.ipynb
â”œâ”€â”€ PLA/
â”‚   â”œâ”€â”€ PLA_report.ipynb
â”‚   â”œâ”€â”€ VoDaiLuong_22520834_LAB1.docx
â”‚   â””â”€â”€ kaggle.json
â”œâ”€â”€ Support Vector Machine/
â”‚   â”œâ”€â”€ SVM.ipynb
â”‚   â”œâ”€â”€ Soft SVM.ipynb
â”‚   â”œâ”€â”€ Kernel SVM.ipynb
â”‚   â”œâ”€â”€ MultiC SVM.ipynb
â”‚   â””â”€â”€ Tá»”NG Há»¢P CÃ”NG THá»¨C SVM.docx
â”œâ”€â”€ Softmax Regression/
â”‚   â”œâ”€â”€ Sofmax Regression.ipynb
â”‚   â””â”€â”€ softmax_tensorflow.ipynb
â”œâ”€â”€ Bias - varience tradeoff/
â”‚   â””â”€â”€ 22520834_BiasVarianceTradeoff.ipynb
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ› ï¸ Technologies Used

- **Python**: Primary programming language
- **Jupyter Notebooks**: Interactive development and documentation
- **NumPy**: Numerical computations
- **Pandas**: Data manipulation and analysis
- **Matplotlib/Seaborn**: Data visualization
- **Scikit-learn**: Comparison with standard implementations
- **TensorFlow**: Deep learning integration (for Softmax)

## ğŸ“Š Datasets Included

### Linear Regression Datasets
- **Tesla Stock Data**: Historical stock prices for time series analysis
- **Fish Dataset**: Fish measurements for regression modeling
- **Insurance Dataset**: Insurance costs prediction

### Other Datasets
- Various datasets for classification tasks
- Synthetic data for algorithm demonstration
- Real-world examples for practical applications

## ğŸš€ Getting Started

### Prerequisites
- Python 3.7+
- Jupyter Notebook
- Required Python packages (see installation section)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/your-username/Machine-Learning-From-Scratch.git
cd Machine-Learning-From-Scratch
```

2. **Install required packages**
```bash
pip install numpy pandas matplotlib seaborn scikit-learn jupyter tensorflow
```

3. **Launch Jupyter Notebook**
```bash
jupyter notebook
```

### Running Examples

1. **Start with Linear Regression**
   - Open `LinearRegression/22520834_LinearRegression_assignment.ipynb`
   - Follow the tutorial and complete the assignment

2. **Explore Classification Algorithms**
   - Try `Logistic Regression/LogisticRegression.ipynb`
   - Experiment with `KNN/KNN.ipynb`

3. **Advanced Algorithms**
   - Study `Support Vector Machine/SVM.ipynb`
   - Understand `Softmax Regression/Sofmax Regression.ipynb`

## ğŸ“– Learning Path

### Beginner Level
1. **Linear Regression** - Start with the fundamentals
2. **Data Preprocessing** - Learn data cleaning and preparation
3. **KNN** - Simple classification algorithm

### Intermediate Level
1. **Logistic Regression** - Binary classification
2. **PLA** - Understanding linear classifiers
3. **Bias-Variance Tradeoff** - Model complexity concepts

### Advanced Level
1. **Support Vector Machine** - Advanced classification
2. **Softmax Regression** - Multi-class classification
3. **Kernel Methods** - Non-linear transformations

## ğŸ¯ Key Features

- **From Scratch Implementation**: All algorithms are implemented without relying on ML libraries
- **Educational Focus**: Detailed explanations and mathematical foundations
- **Practical Examples**: Real-world datasets and applications
- **Interactive Learning**: Jupyter notebooks for hands-on experience
- **Comprehensive Documentation**: Tutorials and formula references

## ğŸ“ˆ Algorithm Highlights

### Linear Regression
- Gradient descent optimization
- Feature scaling and normalization
- Multiple regression with regularization
- Real-world applications with stock data

### Support Vector Machine
- Hard margin vs soft margin classification
- Kernel trick implementation (RBF, Polynomial)
- Multi-class classification strategies
- Parameter tuning and optimization

### K-Nearest Neighbors
- Distance metric implementations
- K-value selection strategies
- Cross-validation techniques
- Performance evaluation

## ğŸ”¬ Research and Development

This repository serves as both an educational resource and a research platform for:
- Algorithm comparison and benchmarking
- Custom algorithm modifications
- Performance optimization studies
- Educational material development

## ğŸ¤ Contributing

We welcome contributions to improve the implementations and add new algorithms!

1. Fork the project
2. Create a feature branch (`git checkout -b feature/NewAlgorithm`)
3. Commit your changes (`git commit -m 'Add new algorithm implementation'`)
4. Push to the branch (`git push origin feature/NewAlgorithm`)
5. Open a Pull Request

### Contribution Guidelines
- Follow the existing code structure
- Include comprehensive documentation
- Add example datasets when applicable
- Provide mathematical explanations
- Include performance comparisons

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Vo Dai Luong**
- GitHub: [@vodailuong2510](https://github.com/vodailuong2510)
- Email: vodailuong2510@gmail.com

## ğŸ™ Acknowledgments

- **UIT (University of Information Technology)**: Academic support and resources
- **Kaggle**: Datasets and learning resources
- **Open Source Community**: Libraries and tools that made this project possible
- **Machine Learning Community**: Continuous inspiration and knowledge sharing

## ğŸ“š Additional Resources

- **Tutorials**: Each algorithm folder contains detailed tutorial materials
- **Formulas**: Comprehensive mathematical documentation in Vietnamese
- **Assignments**: Practical exercises for hands-on learning
- **Research Papers**: References to original algorithm papers

## ğŸ“ Educational Use

This repository is designed for:
- **Students**: Learning machine learning fundamentals
- **Researchers**: Algorithm implementation reference
- **Educators**: Teaching materials and examples
- **Practitioners**: Understanding algorithm internals

---

â­ **Star this repository if you find it helpful for your machine learning journey!**
